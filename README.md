# llm-manager
 Orchestrates setting up ollama docker containers and queueing requests to ensure 1 request per gpu.
